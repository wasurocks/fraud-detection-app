{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Model Training\n",
    "\n",
    "This notebook implements a comprehensive fraud detection model training pipeline.\n",
    "\n",
    "## Overview\n",
    "- Data preprocessing and feature engineering\n",
    "- Model training with multiple algorithms\n",
    "- Hyperparameter tuning\n",
    "- Comprehensive evaluation with fraud-specific metrics\n",
    "- Model interpretation and feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries with error handling\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Starting imports...\")\n\n# Basic libraries\ntry:\n    import pandas as pd\n    import numpy as np\n    import matplotlib\n    matplotlib.use('Agg')  # Use non-interactive backend for Docker\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    print(\"✅ Basic libraries imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing basic libraries: {e}\")\n    raise\n\n# Machine learning libraries\ntry:\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import (\n        classification_report, confusion_matrix, roc_auc_score, \n        precision_recall_curve, roc_curve, average_precision_score,\n        precision_score, recall_score, f1_score\n    )\n    from sklearn.model_selection import cross_val_score, StratifiedKFold\n    print(\"✅ Scikit-learn imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing scikit-learn: {e}\")\n    raise\n\ntry:\n    from imblearn.over_sampling import SMOTE\n    from imblearn.under_sampling import RandomUnderSampler\n    from imblearn.pipeline import Pipeline as ImbPipeline\n    print(\"✅ Imbalanced-learn imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing imbalanced-learn: {e}\")\n    raise\n\ntry:\n    import xgboost as xgb\n    print(\"✅ XGBoost imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing XGBoost: {e}\")\n    raise\n\ntry:\n    import lightgbm as lgb\n    print(\"✅ LightGBM imported successfully\")  \nexcept Exception as e:\n    print(f\"❌ Error importing LightGBM: {e}\")\n    raise\n\ntry:\n    import joblib\n    import optuna\n    from tqdm import tqdm\n    print(\"✅ Utility libraries imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing utility libraries: {e}\")\n    raise\n\n# Custom modules with proper path handling\ntry:\n    # Add src directory to path\n    src_path = '/app/src'\n    if src_path not in sys.path:\n        sys.path.insert(0, src_path)\n    \n    # Check if we can access the module file\n    module_path = os.path.join(src_path, 'data_preprocessing.py')\n    if not os.path.exists(module_path):\n        raise FileNotFoundError(f\"Module file not found: {module_path}\")\n    \n    from data_preprocessing import FraudDataProcessor\n    print(\"✅ Custom data preprocessing module imported successfully\")\nexcept Exception as e:\n    print(f\"❌ Error importing custom modules: {e}\")\n    print(f\"Current working directory: {os.getcwd()}\")\n    print(f\"Python path: {sys.path[:3]}...\")\n    if os.path.exists('/app/src'):\n        print(f\"Contents of /app/src: {os.listdir('/app/src')}\")\n    raise\n\n# Settings\npd.set_option('display.max_columns', None)\nplt.style.use('default')  # Use default style for better Docker compatibility\nnp.random.seed(42)\n\nprint(\"✅ Setup completed successfully!\")\nprint(\"✅ All imports working correctly!\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Available memory: {os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024.**3):.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the data processor and load data with error handling\nimport gc\nimport psutil\n\nprint(\"Checking system resources...\")\nmemory = psutil.virtual_memory()\nprint(f\"Available memory: {memory.available / (1024**3):.1f} GB\")\nprint(f\"Memory usage: {memory.percent:.1f}%\")\n\ntry:\n    # Initialize the data processor\n    processor = FraudDataProcessor()\n    print(\"✅ Data processor initialized successfully\")\n    \n    # Check if data file exists\n    data_path = \"/app/data/fraud_mock.csv\"\n    if not os.path.exists(data_path):\n        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n    \n    # Get file size\n    file_size = os.path.getsize(data_path) / (1024 * 1024)  # MB\n    print(f\"Data file size: {file_size:.1f} MB\")\n    \n    # Process the data with temporal split (recommended for fraud detection)\n    print(\"Starting data preprocessing pipeline...\")\n    print(\"This may take a few minutes for large datasets...\")\n    \n    train_df, val_df, test_df = processor.process_pipeline(\n        file_path=data_path,\n        test_size=0.2,\n        val_size=0.1,\n        temporal_split=True\n    )\n    \n    print(\"✅ Data preprocessing completed successfully!\")\n    print(f\"Training set shape: {train_df.shape}\")\n    print(f\"Validation set shape: {val_df.shape}\")\n    print(f\"Test set shape: {test_df.shape}\")\n    \n    # Check memory usage after loading\n    memory_after = psutil.virtual_memory()\n    print(f\"Memory usage after loading: {memory_after.percent:.1f}%\")\n    \n    # Force garbage collection\n    gc.collect()\n    print(\"✅ Garbage collection completed\")\n\nexcept FileNotFoundError as e:\n    print(f\"❌ Data file error: {e}\")\n    raise\nexcept MemoryError as e:\n    print(f\"❌ Not enough memory to load the full dataset: {e}\")\n    print(\"Try reducing the dataset size or increasing Docker memory limits\")\n    raise\nexcept Exception as e:\n    print(f\"❌ Error during data preprocessing: {e}\")\n    print(\"This might be due to:\")\n    print(\"- Insufficient memory\")\n    print(\"- Corrupted data file\") \n    print(\"- Missing dependencies\")\n    raise"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "feature_columns = [col for col in train_df.columns if col not in ['is_fraud', 'is_flagged_fraud']]\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['is_fraud']\n",
    "\n",
    "X_val = val_df[feature_columns]\n",
    "y_val = val_df['is_fraud']\n",
    "\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['is_fraud']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Training fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Validation fraud rate: {y_val.mean():.4f}\")\n",
    "print(f\"Test fraud rate: {y_test.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Analysis and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance\n",
    "fraud_counts = y_train.value_counts()\n",
    "fraud_ratio = fraud_counts[1] / fraud_counts[0]\n",
    "\n",
    "print(f\"Non-fraud transactions: {fraud_counts[0]:,}\")\n",
    "print(f\"Fraud transactions: {fraud_counts[1]:,}\")\n",
    "print(f\"Fraud ratio: 1:{fraud_counts[0]//fraud_counts[1]}\")\n",
    "print(f\"Imbalance ratio: {fraud_ratio:.6f}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "y_train.value_counts().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Training Set Class Distribution')\n",
    "axes[0].set_xlabel('Class (0: Non-fraud, 1: Fraud)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Percentage plot\n",
    "(y_train.value_counts(normalize=True) * 100).plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Training Set Class Distribution (%)')\n",
    "axes[1].set_xlabel('Class (0: Non-fraud, 1: Fraud)')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target\n",
    "feature_target_corr = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'correlation': [train_df[feature].corr(train_df['is_fraud']) for feature in feature_columns]\n",
    "})\n",
    "\n",
    "feature_target_corr = feature_target_corr.sort_values('correlation', key=abs, ascending=False)\n",
    "print(\"Top 15 features by correlation with fraud:\")\n",
    "print(feature_target_corr.head(15))\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_target_corr.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['correlation'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Correlation with Fraud')\n",
    "plt.title('Top 20 Features by Correlation with Fraud')\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation for fraud detection.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    results['precision'] = precision_score(y_true, y_pred)\n",
    "    results['recall'] = recall_score(y_true, y_pred)\n",
    "    results['f1'] = f1_score(y_true, y_pred)\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        results['auc_roc'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        results['auc_pr'] = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    results['true_negatives'] = tn\n",
    "    results['false_positives'] = fp\n",
    "    results['false_negatives'] = fn\n",
    "    results['true_positives'] = tp\n",
    "    \n",
    "    # Cost-sensitive metrics (assuming FP costs 1 and FN costs 10)\n",
    "    fp_cost = 1\n",
    "    fn_cost = 10\n",
    "    results['total_cost'] = fp * fp_cost + fn * fn_cost\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1']:.4f}\")\n",
    "    if 'auc_roc' in results:\n",
    "        print(f\"AUC-ROC: {results['auc_roc']:.4f}\")\n",
    "        print(f\"AUC-PR: {results['auc_pr']:.4f}\")\n",
    "    print(f\"Total Cost (FP:1, FN:10): {results['total_cost']}\")\n",
    "    \n",
    "    # Confusion matrix visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curve.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    auc_pr = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'{model_name} (AUC-PR = {auc_pr:.4f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC-ROC = {auc_roc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline logistic regression with class balancing\n",
    "print(\"Training Baseline Logistic Regression...\")\n",
    "\n",
    "# Create pipeline with SMOTE for handling class imbalance\n",
    "smote_lr_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "smote_lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_lr = smote_lr_pipeline.predict(X_val)\n",
    "y_val_pred_proba_lr = smote_lr_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "lr_results = evaluate_model(y_val, y_val_pred_lr, y_val_pred_proba_lr, \"Logistic Regression (SMOTE)\")\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_lr, \"Logistic Regression (SMOTE)\")\n",
    "plot_roc_curve(y_val, y_val_pred_proba_lr, \"Logistic Regression (SMOTE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with class balancing\n",
    "print(\"Training Random Forest...\")\n",
    "\n",
    "# Random Forest with class weight balancing\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_val_pred_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "rf_results = evaluate_model(y_val, y_val_pred_rf, y_val_pred_proba_rf, \"Random Forest\")\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_rf, \"Random Forest\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features (Random Forest):\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features_rf = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features_rf)), top_features_rf['importance'])\n",
    "plt.yticks(range(len(top_features_rf)), top_features_rf['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features by Random Forest Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. XGBoost Model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameter tuning with Optuna\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "print(\"Starting XGBoost hyperparameter tuning...\")\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best AUC-ROC: {study_xgb.best_value:.4f}\")\n",
    "print(f\"Best parameters: {study_xgb.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final XGBoost model with best parameters\n",
    "print(\"Training final XGBoost model...\")\n",
    "\n",
    "# Add scale_pos_weight to best params\n",
    "best_params_xgb = study_xgb.best_params.copy()\n",
    "best_params_xgb['scale_pos_weight'] = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "best_params_xgb['random_state'] = 42\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**best_params_xgb)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "y_val_pred_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "xgb_results = evaluate_model(y_val, y_val_pred_xgb, y_val_pred_proba_xgb, \"XGBoost (Tuned)\")\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_xgb, \"XGBoost (Tuned)\")\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features (XGBoost):\")\n",
    "print(xgb_feature_importance.head(15))\n",
    "\n",
    "# Plot XGBoost feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features_xgb = xgb_feature_importance.head(20)\n",
    "plt.barh(range(len(top_features_xgb)), top_features_xgb['importance'])\n",
    "plt.yticks(range(len(top_features_xgb)), top_features_xgb['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features by XGBoost Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "print(\"Training LightGBM...\")\n",
    "\n",
    "# Calculate class weights\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_lgb = lgb_model.predict(X_val)\n",
    "y_val_pred_proba_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "lgb_results = evaluate_model(y_val, y_val_pred_lgb, y_val_pred_proba_lgb, \"LightGBM\")\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_lgb, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (SMOTE)', 'Random Forest', 'XGBoost (Tuned)', 'LightGBM'],\n",
    "    'Precision': [lr_results['precision'], rf_results['precision'], \n",
    "                 xgb_results['precision'], lgb_results['precision']],\n",
    "    'Recall': [lr_results['recall'], rf_results['recall'], \n",
    "              xgb_results['recall'], lgb_results['recall']],\n",
    "    'F1-Score': [lr_results['f1'], rf_results['f1'], \n",
    "                xgb_results['f1'], lgb_results['f1']],\n",
    "    'AUC-ROC': [lr_results['auc_roc'], rf_results['auc_roc'], \n",
    "               xgb_results['auc_roc'], lgb_results['auc_roc']],\n",
    "    'AUC-PR': [lr_results['auc_pr'], rf_results['auc_pr'], \n",
    "              xgb_results['auc_pr'], lgb_results['auc_pr']],\n",
    "    'Total Cost': [lr_results['total_cost'], rf_results['total_cost'], \n",
    "                  xgb_results['total_cost'], lgb_results['total_cost']]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison on Validation Set:\")\n",
    "print(model_comparison.round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//3, i%3]\n",
    "    model_comparison.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Total cost comparison (lower is better)\n",
    "ax = axes[1, 2]\n",
    "model_comparison.plot(x='Model', y='Total Cost', kind='bar', ax=ax, legend=False, color='red')\n",
    "ax.set_title('Total Cost Comparison (Lower is Better)')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Model Selection and Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on AUC-PR (most important for imbalanced fraud detection)\n",
    "best_model_idx = model_comparison['AUC-PR'].idxmax()\n",
    "best_model_name = model_comparison.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"Best model based on AUC-PR: {best_model_name}\")\n",
    "\n",
    "# Map model name to actual model object\n",
    "models_dict = {\n",
    "    'Logistic Regression (SMOTE)': smote_lr_pipeline,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost (Tuned)': xgb_model,\n",
    "    'LightGBM': lgb_model\n",
    "}\n",
    "\n",
    "best_model = models_dict[best_model_name]\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\nEvaluating {best_model_name} on test set...\")\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "final_results = evaluate_model(y_test, y_test_pred, y_test_pred_proba, f\"{best_model_name} (Test Set)\")\n",
    "plot_precision_recall_curve(y_test, y_test_pred_proba, f\"{best_model_name} (Test Set)\")\n",
    "plot_roc_curve(y_test, y_test_pred_proba, f\"{best_model_name} (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis\n",
    "print(\"Business Impact Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test set statistics\n",
    "total_transactions = len(y_test)\n",
    "actual_frauds = y_test.sum()\n",
    "actual_legit = total_transactions - actual_frauds\n",
    "\n",
    "print(f\"Total test transactions: {total_transactions:,}\")\n",
    "print(f\"Actual fraud transactions: {actual_frauds:,} ({actual_frauds/total_transactions*100:.2f}%)\")\n",
    "print(f\"Actual legitimate transactions: {actual_legit:,}\")\n",
    "\n",
    "# Model predictions\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"True Positives (Correctly identified frauds): {tp:,}\")\n",
    "print(f\"False Negatives (Missed frauds): {fn:,}\")\n",
    "print(f\"True Negatives (Correctly identified legitimate): {tn:,}\")\n",
    "print(f\"False Positives (Incorrectly flagged legitimate): {fp:,}\")\n",
    "\n",
    "# Business metrics\n",
    "fraud_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(f\"\\nKey Business Metrics:\")\n",
    "print(f\"Fraud Detection Rate: {fraud_detection_rate:.2%} ({tp} out of {tp+fn} frauds caught)\")\n",
    "print(f\"False Alarm Rate: {false_alarm_rate:.2%} ({fp} out of {fp+tn} legitimate transactions flagged)\")\n",
    "\n",
    "# Cost analysis\n",
    "avg_fraud_amount = 1000  # Assumed average fraud amount\n",
    "review_cost = 10  # Cost to manually review a flagged transaction\n",
    "\n",
    "# Savings from caught frauds\n",
    "fraud_savings = tp * avg_fraud_amount\n",
    "\n",
    "# Cost of manual reviews\n",
    "review_costs = (tp + fp) * review_cost\n",
    "\n",
    "# Cost of missed frauds\n",
    "missed_fraud_cost = fn * avg_fraud_amount\n",
    "\n",
    "net_benefit = fraud_savings - review_costs - missed_fraud_cost\n",
    "\n",
    "print(f\"\\nCost-Benefit Analysis (Estimated):\")\n",
    "print(f\"Fraud amount prevented: ${fraud_savings:,.2f}\")\n",
    "print(f\"Manual review costs: ${review_costs:,.2f}\")\n",
    "print(f\"Missed fraud losses: ${missed_fraud_cost:,.2f}\")\n",
    "print(f\"Net benefit: ${net_benefit:,.2f}\")\n",
    "\n",
    "# Implications\n",
    "print(f\"\\nImplications:\")\n",
    "print(f\"• False Positives: {fp:,} legitimate customers may experience inconvenience\")\n",
    "print(f\"• False Negatives: {fn:,} fraudulent transactions will go undetected\")\n",
    "print(f\"• The model catches {fraud_detection_rate:.1%} of all fraud attempts\")\n",
    "print(f\"• Only {false_alarm_rate:.1%} of legitimate transactions are incorrectly flagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Interpretation and Feature Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based model\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "elif hasattr(best_model, 'named_steps') and 'classifier' in best_model.named_steps:\n",
    "    # Pipeline model (like SMOTE + LogisticRegression)\n",
    "    if hasattr(best_model.named_steps['classifier'], 'coef_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': np.abs(best_model.named_steps['classifier'].coef_[0])\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"Top 20 Most Important Features for {best_model_name}:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_20_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20_features)), top_20_features['importance'])\n",
    "plt.yticks(range(len(top_20_features)), top_20_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top 20 Features by Importance - {best_model_name}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature insights\n",
    "print(\"\\nKey Feature Insights:\")\n",
    "print(\"=\" * 30)\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "    print(f\"{i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Provide business interpretation for key features\n",
    "    feature_name = row['feature']\n",
    "    if 'amount' in feature_name.lower():\n",
    "        print(\"   → Transaction amount patterns are key indicators of fraud\")\n",
    "    elif 'balance' in feature_name.lower():\n",
    "        print(\"   → Account balance changes help identify suspicious activity\")\n",
    "    elif 'time' in feature_name.lower() or 'hour' in feature_name.lower() or 'day' in feature_name.lower():\n",
    "        print(\"   → Temporal patterns reveal fraud timing preferences\")\n",
    "    elif 'acc' in feature_name.lower() and 'frequency' in feature_name.lower():\n",
    "        print(\"   → Account usage frequency indicates normal vs suspicious behavior\")\n",
    "    elif 'cash_out' in feature_name.lower() or 'transfer' in feature_name.lower():\n",
    "        print(\"   → Transaction type is a strong fraud predictor\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Persistence and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing components\n",
    "import os\n",
    "\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, 'best_fraud_model.joblib')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "# Save the data processor (with fitted scalers and encoders)\n",
    "processor_path = os.path.join(model_dir, 'data_processor.joblib')\n",
    "joblib.dump(processor, processor_path)\n",
    "print(f\"Data processor saved to: {processor_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = os.path.join(model_dir, 'feature_names.joblib')\n",
    "joblib.dump(feature_columns, feature_names_path)\n",
    "print(f\"Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'features_count': len(feature_columns),\n",
    "    'test_performance': final_results,\n",
    "    'feature_importance': feature_importance.head(10).to_dict('records'),\n",
    "    'training_data_size': len(X_train),\n",
    "    'validation_data_size': len(X_val),\n",
    "    'test_data_size': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, 'model_metadata.joblib')\n",
    "joblib.dump(model_metadata, metadata_path)\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(\"\\nModel artifacts saved successfully!\")\n",
    "print(f\"All files saved in: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FRAUD DETECTION MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. DATASET:\")\n",
    "print(f\"   • Total transactions: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"   • Fraud rate: {(y_train.sum() + y_val.sum() + y_test.sum()) / (len(y_train) + len(y_val) + len(y_test)):.4%}\")\n",
    "print(f\"   • Features engineered: {len(feature_columns)}\")\n",
    "\n",
    "print(f\"\\n2. METHODOLOGY:\")\n",
    "print(f\"   • Temporal train/test split (recommended for fraud detection)\")\n",
    "print(f\"   • Class imbalance handled with SMOTE and class weights\")\n",
    "print(f\"   • Multiple algorithms tested: LR, RF, XGBoost, LightGBM\")\n",
    "print(f\"   • Hyperparameter optimization with Optuna\")\n",
    "\n",
    "print(f\"\\n3. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   • Test Set Performance:\")\n",
    "print(f\"     - Precision: {final_results['precision']:.4f}\")\n",
    "print(f\"     - Recall: {final_results['recall']:.4f}\")\n",
    "print(f\"     - F1-Score: {final_results['f1']:.4f}\")\n",
    "print(f\"     - AUC-ROC: {final_results['auc_roc']:.4f}\")\n",
    "print(f\"     - AUC-PR: {final_results['auc_pr']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. KEY FINDINGS:\")\n",
    "print(f\"   • Top fraud indicators: {', '.join(feature_importance.head(3)['feature'].tolist())}\")\n",
    "print(f\"   • Model catches {fraud_detection_rate:.1%} of fraud attempts\")\n",
    "print(f\"   • False alarm rate: {false_alarm_rate:.1%}\")\n",
    "print(f\"   • Estimated net benefit: ${net_benefit:,.2f}\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS:\")\n",
    "print(f\"   • Deploy model with real-time scoring capability\")\n",
    "print(f\"   • Implement model monitoring and periodic retraining\")\n",
    "print(f\"   • Set up feedback loop for false positive/negative learning\")\n",
    "print(f\"   • Consider ensemble methods for improved performance\")\n",
    "print(f\"   • Regularly update features based on new fraud patterns\")\n",
    "\n",
    "print(f\"\\n6. BUSINESS IMPACT:\")\n",
    "print(f\"   • {tp:,} fraudulent transactions will be caught\")\n",
    "print(f\"   • {fn:,} fraudulent transactions will be missed\")\n",
    "print(f\"   • {fp:,} legitimate transactions will require manual review\")\n",
    "print(f\"   • Significant cost savings expected from fraud prevention\")\n",
    "\n",
    "print(f\"\\nTraining completed successfully!\")\n",
    "print(f\"Model artifacts saved for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}